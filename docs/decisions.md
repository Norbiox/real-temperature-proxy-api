# Initial Implementation Decisions

This document summarizes all initial key decisions for the Real Temperature Proxy API implementation.

## API Design & Interface

- **Endpoint**: Use query parameters (`/v1/current?lat=...&lon=...`), not path parameters.
- **Version**: Include API version from the start (`/v1/`).
- **OpenAPI**: Auto-generated by FastAPI; Swagger UI exposed in development only.
- **Content**: JSON only; validate Accept headers.
- **Query parameter case sensitivity**: Case-insensitive; `?LAT=52.5`, `?Lat=52.5`, and `?lat=52.5` all work (normalize to lowercase internally).
- **Open-Meteo parameters**: Hardcoded to fetch `current=temperature_2m,wind_speed_10m`. Not configurable per-request.

## Input Validation & Constraints

- **Coordinates**: Latitude [-90, 90], Longitude [-180, 180].
- **Precision**: Validate max 6 decimal places.
- **Invalid coordinates**: Return 400 Bad Request with error details in JSON format.
- **Parameters**: Both latitude and longitude required. Accept query parameters in this order of precedence:
  1. If both `lat` and `latitude` provided, use `lat`; same for `lon`/`longitude`
  2. Return 400 if both old and new names conflict (e.g., `lat=52.5` AND `latitude=52.6`)
  3. Support both naming schemes for backward compatibility during transition if needed

## Error Handling & Resilience

- **Error response format**: JSON structure: `{"error": "message"}`. Generic messages only, no error codes or details.
- **Upstream errors (4xx, 5xx)**: Return 502 Bad Gateway.
- **Upstream timeout**: Return 504 Gateway Timeout.
- **Retry logic**: Use tenacity library with exponential backoff (2x multiplier) + jitter. Default: 3 retries, initial delay 100ms. Retry on:
  - Timeout (connection or read timeout)
  - Connection refused
  - 5xx errors from upstream
  - Do NOT retry on: 4xx errors, DNS failures
  - Configurable via `RETRY_COUNT`, `RETRY_DELAY`, `RETRY_BACKOFF_MULTIPLIER`
- **Error caching**: Do NOT cache error responses (502, 503, 504). Every request goes upstream when not cached (retry opportunity).
- **Network errors**: Return 502 Bad Gateway; no differentiation in response, but detailed logging.
- **Request coalescing**: When multiple concurrent requests arrive for same coordinates (cache miss), first request fetches upstream via tenacity with retries; others wait. Implementation: `asyncio.Event` per coordinate. Limit to 100 concurrent waiters per coordinate; excess requests get 503 Service Unavailable to prevent unbounded memory growth.

## Caching Strategy

- **Implementation**: fastapi-cache2 with in-memory backend.
- **TTL**: 60 seconds.
- **Max size**: 10,000 entries (configurable via environment variable). Use LRU eviction when capacity reached.
- **Eviction sequence**: When cache is full and new request arrives: evict oldest (LRU) entry → fetch upstream → cache result.
- **Coordinate rounding**: Round to 4 decimal places using Python's `round()` function (banker's rounding) for cache key; return result with original precision.
- **Multi-worker**: Each uvicorn worker has its own cache (acceptable for K8s horizontal scaling).
- **Sticky sessions**: Use in load balancer due to per-pod cache. Note: This creates operational fragility—pod restarts cause cache loss. Consider Redis backend for distributed cache if horizontal scaling/HA becomes critical.

## Response Format & Data Normalization

- **Timestamps**: ISO 8601 UTC format; `retrievedAt` reflects original fetch time (not current time when serving from cache).
- **Temperature & Wind Speed**: 1 decimal place, rounded using Python's default `round()` function (banker's rounding).
- **Float representation**: Use JSON numbers (`1.2`), not strings (`"1.2"`).
- **Missing data**: Return `null` for fields not available from Open-Meteo. Always include both `temperatureC` and `windSpeedKmh` fields, even if null.
  - Example: `{"temperatureC": 1.2, "windSpeedKmh": null}`
- **No additional metadata**: No cache status, response time, or upstream metadata in response.

## Configuration Management

- **Worker count**: Fixed at 1 uvicorn worker (K8s pattern: scale via pod replicas, not worker processes).
- **Environment variables** (with defaults):
  - `UPSTREAM_TIMEOUT`: 1 second
  - `CACHE_TTL`: 60 seconds
  - `CACHE_MAX_SIZE`: 10,000
  - `LOG_LEVEL`: INFO
  - `OPENMETEO_BASE_URL`: https://api.open-meteo.com/v1/forecast
  - `PORT`: 8000
  - `RETRY_COUNT`: 3 (max retries on upstream failure)
  - `RETRY_DELAY`: 100 (initial delay in ms)
  - `RETRY_BACKOFF_MULTIPLIER`: 2 (exponential backoff: 100ms, 200ms, 400ms)
  - `OPENMETEO_API_KEY`: (optional; only passed to upstream if set)
  - `REQUEST_COALESCE_LIMIT`: 100 (max concurrent waiters per coordinate)
- **Validation**: Validate configuration at startup using Pydantic; fail fast if invalid. Log active configuration at startup (without sensitive values like API keys).

## Health Checks & Monitoring

- **Endpoints**: Implement `/health` (liveness) and `/ready` (readiness).
- **Liveness** (`/health`): Always return 200 with body `{"status": "ok"}`. Check only if app process is running.
- **Readiness** (`/ready`): Return 200 with body `{"status": "ok"}` only after cache backend is initialized. Check internal state (cache working, configuration valid). Do NOT actively probe Open-Meteo.
- **Readiness timing**: `/ready` starts returning 200 after cache backend initialization completes (not after config load, not after first upstream call).
- **Upstream failure handling**: Return 503 Service Unavailable with error body `{"error": "..."}`. Don't cascade to pod restarts.
- **Metrics**: Use OpenTelemetry for instrumentation (vendor-agnostic, supports multiple backends). Expose metrics at `/metrics` endpoint compatible with Prometheus scraping. Open to everyone (no authentication).
- **Metrics to track**:
  - Request count (by status code, by endpoint)
  - Request duration (histogram with buckets: 10ms, 50ms, 100ms, 500ms, 1s, 1.5s)
  - Cache hit/miss ratio (implement custom metric if fastapi-cache2 doesn't expose it)
  - Upstream API latency
  - Upstream API error rate (by error type: timeout, 4xx, 5xx)
- **Logging**: Use loguru for structured logging:
  - Do NOT log coordinates in normal operation (PII concern). Exception: Log coordinates when Open-Meteo fails to return a result for incident investigation (acceptable trade-off).
  - Log upstream API calls and responses (without coordinates; log status codes and latency).
  - Log errors with full stack traces.
  - **Log level determination**: Set via `LOG_LEVEL` environment variable. Default: `INFO`. At startup, log which level is active to confirm configuration.

## Security Considerations

- **Rate limiting**: Configure at Ingress controller level, not in application code.
- **CORS**: Enable for all origins (`*`); hardcoded, not configurable via environment variable.
- **Authentication**: No authentication required; public API.
- **Input sanitization**: No special sanitization beyond coordinate validation.
- **Security headers**:
  - In application: CORS headers only.
  - At Ingress level: X-Content-Type-Options: nosniff, Strict-Transport-Security (HSTS).
  - Skip: X-Frame-Options, CSP, X-XSS-Protection (not relevant for JSON APIs).

## Open-Meteo Integration

- **API key**: Optional; support via environment variable. Pass only if set.
- **Rate limits**: Free plan: 600/min, 5000/hour, 10000/day, 300000/month. Respect in caching, not explicitly.
- **Response format**: Reference docs/open-meteo-response.json for exact structure.
- **Version pinning**: No; handle upstream API changes via integration tests with vcrpy cassettes.

## Containerization & Deployment

- **Docker base image**: `python:3.14-slim`.
- **Multi-stage build**: Yes; exclude dev dependencies.
- **Non-root user**: Yes.
- **Kubernetes manifests**: Include examples (Deployment, Service, Ingress).
- **Resource limits**:
  - CPU: request 100m, limit 500m
  - Memory: request 128Mi, limit 512Mi
- **Replicas**: 2 for HA.
- **Health probe settings**:
  - Readiness: initialDelaySeconds 30, periodSeconds 10, timeoutSeconds 5
  - Liveness: initialDelaySeconds 30, periodSeconds 10, timeoutSeconds 5

## Testing Strategy

- **Coverage**: Aim for 100% branch coverage; enforce in CI.
- **Integration tests**: Use vcrpy for recording/replaying Open-Meteo responses:
  - Run unit tests + mocked integration tests in CI on every commit
  - Run full integration tests (with vcrpy cassettes) on scheduled nightly job to catch upstream API changes
  - Manual integration testing before releases to verify cassettes still match actual API behavior
- **Test coordinates**: Use consistent, deterministic coordinates (e.g., Berlin 52.52,13.41; Sydney -33.87,151.21); test multiple coordinates for caching/eviction.
- **Test scenarios**: Valid coordinates, cache hits/misses, upstream timeout, upstream errors, invalid coordinates, concurrent requests, cache eviction, request coalescing.
- **Load testing**: Not in CI pipeline; available for manual on-demand testing before production deployment.

## CI/CD & Development Workflow

- **Pre-commit hooks**: Run fast subset of pytest with doctest, ruff check/format (with --fix), bandit, mypy; fail fast.
- **GitHub Actions**: Test only Python 3.14 (initially; can add matrix testing for other versions later); run unit tests + mocked integration tests on every push; don't build/push Docker images in CI.
- **Commit strategy**: Trunk-based on master; very granular commits, TDD style; each commit must pass tests.
- **Merge strategy**: Rebase and merge; keep history.

## Documentation Requirements

- **README**: Quick start, API usage examples (curl), environment variable reference, development setup, deployment instructions.
- **Additional docs**: No extra API documentation or architecture diagrams.
- **Code documentation**: Google-style docstrings with doctest when applicable for all public functions.

## Future Extensibility

- **Additional metrics**: Not extensible now, but note for future consideration.
- **Multiple data sources**: Single Open-Meteo only; no abstraction layer.
- **Historical data**: Focus on current conditions only.

## Performance & Scalability

- **Optimization target**: Low latency (not throughput).
- **Response time SLAs**:
  - Cached: p95 < 50ms
  - Uncached: p95 < 1.5s
  - Throughput: ≥ 1000 req/s (naturally achieved through caching)
- **Concurrent connections**: Accept thousands via async framework.
- **Request handling**: Implement request deduplication (request coalescing). Multiple concurrent requests for same coordinates wait for single upstream fetch, preventing thundering herd and rate limit exhaustion. Limit to 100 waiters per coordinate to prevent unbounded memory growth.

## Deployment & Operations

- **Graceful shutdown**: On SIGTERM, stop accepting new requests but allow 30s for in-flight requests to complete before force shutdown.
- **Readiness during startup**: Application becomes ready (returns 200 on `/ready`) only after configuration is loaded and validated.
- **Cache behavior during pod termination**: Cache is lost on pod restart (acceptable given 60s TTL and high hit rate from geographic clustering).
- **Future optimization**: If horizontal scaling becomes problematic due to per-pod cache, migrate to Redis backend. Current design supports this upgrade path without code changes (fastapi-cache2 supports multiple backends).
